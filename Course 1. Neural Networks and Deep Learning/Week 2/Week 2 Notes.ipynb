{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Cost Function\n",
    "## Logistic Regression Model\n",
    "- $\\hat y = \\sigma (w^{T}X + b)$, where $\\sigma (z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "## Loss Function\n",
    "- The loss function computes the error for a single training example.\n",
    "- $L(\\hat y, y) = -(ylog(\\hat y) + (1-y)log(1-\\hat y))$\n",
    "\n",
    "## Cost Function\n",
    "- The cost function is the average of the loss functions of the entire training set.\n",
    "- $J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat y^{(i)}, y^{(i)}) = -\\frac{1}{m} \\sum_{i=1}^{m} y^{(i)}log(\\hat y^{(i)}) + (1-y^{(i)})log(1-\\hat y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "- We want to find $(w, b)$ that minimize $J(w, b)$.\n",
    "- Note that the cost function needs to be a convex function.\n",
    "- Repeat:\n",
    "    - $w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}$\n",
    "        - For coding convention, use `dw` to represent the term $\\frac{\\partial J(w, b)}{\\partial w}$.\n",
    "    - $b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}$\n",
    "        - For coding convention, use `db` to represent the term $\\frac{\\partial J(w, b)}{\\partial b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives with a Computation Graph\n",
    "- Use chain rule to derive partial derivatives of the final output variable with respect to various intermediate quantities.\n",
    "- For conding convension, use `dvar` to represent $\\frac{\\partial Var_{final}}{\\partial Var_{intermediate}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Gradient Descent (on *one* Example)\n",
    "- Logistic Regression Recap\n",
    "    - $z = w^{T} X + b$\n",
    "        - E.g. $z = w_{1}x_{1} + w_{2}x_{2} + b$\n",
    "    - $\\hat y = a = \\sigma (z)$\n",
    "    - $L(a, y) = -(ylog(a) + (1-y)log(1-a))$\n",
    "- Logistic Regression Derivatives\n",
    "    - $da = \\frac{\\partial L(a, y)}{\\partial a} = -\\frac{y}{a} + \\frac{1-y}{1-a}$\n",
    "    - $dz = \\frac{\\partial L(a, y)}{\\partial z} = \\frac{\\partial L(a, y)}{\\partial a} \\times \\frac{\\partial a}{\\partial z} = a-y$\n",
    "    - $dw_{i} = \\frac{\\partial L(a, y)}{\\partial w_{i}} = x_{i}dz$\n",
    "    - $db = \\frac{\\partial L(a, y)}{\\partial b} = dz$\n",
    "- Then, for Gradient Descent:\n",
    "    - $w_{i} := w_{i} - \\alpha dw_{i}$\n",
    "    - $b := b - \\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Gradient Descent (on *m* Examples)\n",
    "- Logistic Regression Recap\n",
    "    - $z = w^{T} X + b$\n",
    "    - $\\hat y = a = \\sigma (z)$\n",
    "    - $L(a, y) = -(ylog(a) + (1-y)log(1-a))$\n",
    "    - $J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} L(a^{(i)}, y^{(i)})$\n",
    "- Gradient Descent on *m* examples:\n",
    "    - For $i = 1 : m$:\n",
    "        - $z^{(i)} = w^{T}X^{(i)} + b$\n",
    "        - $a^{(i)} = \\sigma (z^{(i)})$\n",
    "        - $J = J + L(a^{(i)}, y^{(i)})$\n",
    "        - $dz^{(i)} = a^{(i)} - y^{(i)}$\n",
    "        - $dw_{j} = dw_{j} + x_{j}^{(i)}dz^{(i)}$\n",
    "        - $db = db + dz^{(i)}$\n",
    "    - $J = \\frac{J}{m}$\n",
    "    - $dw_{j} = \\frac{dw_{j}}{m}$\n",
    "    - $db = \\frac{db}{m}$\n",
    "    - $w_{j} := w_{j} - \\alpha dw_{j}$\n",
    "    - $b := b - \\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "- Vectorization can be used on both CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3,4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249962.74545114438\n",
      "Vectorized Version: 0.957489013671875ms\n",
      "249962.74545113952\n",
      "For Loop: 715.6250476837158ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "\n",
    "print(\"Vectorized Version: \" + str(1000*(toc - tic)) + 'ms')\n",
    "\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "\n",
    "print(\"For Loop: \" + str(1000*(toc - tic)) + 'ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Logistic Regression Gradient Descent on *m* examples\n",
    "- For-Loop Style:\n",
    "    - For $i = 1 : m$:\n",
    "        - $z^{(i)} = w^{T}X^{(i)} + b$\n",
    "        - $a^{(i)} = \\sigma (z^{(i)})$\n",
    "        - $J = J + L(a^{(i)}, y^{(i)})$\n",
    "        - $dz^{(i)} = a^{(i)} - y^{(i)}$\n",
    "        - $dw_{j} = dw_{j} + x_{j}^{(i)}dz^{(i)}$\n",
    "        - $db = db + dz^{(i)}$\n",
    "    - $J = \\frac{J}{m}$\n",
    "    - $dw_{j} = \\frac{dw_{j}}{m}$\n",
    "    - $db = \\frac{db}{m}$\n",
    "    - $w_{j} := w_{j} - \\alpha dw_{j}$\n",
    "    - $b := b - \\alpha db$\n",
    "- With vectorization, we can write:\n",
    "    - Initilization:\n",
    "        - $dw = np.zeros((n_{j}, 1))$\n",
    "    - With in the loop:\n",
    "        - $dw = dw + X^{(i)}dz^{(i)}$\n",
    "    - After the loop:\n",
    "        - $dw = dw / m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Logistic Regression\n",
    "- $Z = [z^{(1)}, z^{(1)}, ..., z^{(m)}] = w^{T}X + [b, b, ..., b] = [w^{T}X^{(1)}, w^{T}X^{(2)}, ..., w^{T}X^{(m)}]$\n",
    "    - $Z = np.dot(W.T, X) + b$\n",
    "- $A = [a^{(1)}, a^{(2)}, ..., a^{(m)}] = \\sigma (Z)$\n",
    "    - $A = 1/(1 + np.exp(-Z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Logistic Regression's Gradient Output\n",
    "- $dZ = [dz^{(1)}, dz^{(2)}, ..., dz^{(m)}]$\n",
    "    - $dZ = A - Y$\n",
    "- $db = \\frac{1}{m} \\sum_{i=1}^{m}dz^{(i)}$\n",
    "    - $db = np.sum(dZ) / m$\n",
    "- $dw = \\frac{1}{m} X dZ^{T}$\n",
    "    - $dw = np.dot(X, dZ.T)/m$\n",
    "    \n",
    "## One Iteration of Vectorized Gradient Descent\n",
    "- $Z = np.dot(W.T, X) + b$\n",
    "- $A = \\sigma(Z) = 1/(1 + np.exp(-Z))$\n",
    "- $dZ = A - Y$\n",
    "- $dw = np.dot(X, dZ.T)/m$\n",
    "- $db = np.sum(dZ) / m$\n",
    "- $w := w - \\alpha dw$\n",
    "- $b := b - \\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting in Python\n",
    "- General Principal with Numpy\n",
    "    - $(m, n) \\langle operation \\rangle c ==> (m, n)$, element-wise operation\n",
    "    - $(m, n) \\langle operation \\rangle (1, n) ==> (m, n)$, same operation for each row\n",
    "    - $(m, n) \\langle operation \\rangle (m, 1) ==> (m, n)$, same operation for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Note on Python/Numpy Vectors\n",
    "- When intiating numpy arrays, it's possible to excute the code below.\n",
    "- The issue with vectors like this is that they are **rank 1** vectors, which doesn't work consistently as either a row or column vector.\n",
    "- E.g. `np.dot` will return inner products rather than matrix products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.72406734  0.28408294  1.49804334 -0.16042441  0.10745449]\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(5)\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.094294974584723"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a, a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To return a $5 \\times 5$ matrix product, we should do below instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99816932]\n",
      " [-1.21375599]\n",
      " [ 0.77200461]\n",
      " [-0.25038699]\n",
      " [-0.02744575]]\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(5, 1)\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.96341997e-01, -1.21153400e+00,  7.70591320e-01,\n",
       "        -2.49928609e-01, -2.73955094e-02],\n",
       "       [-1.21153400e+00,  1.47320361e+00, -9.37025225e-01,\n",
       "         3.03908706e-01,  3.33124481e-02],\n",
       "       [ 7.70591320e-01, -9.37025225e-01,  5.95991120e-01,\n",
       "        -1.93299908e-01, -2.11882484e-02],\n",
       "       [-2.49928609e-01,  3.03908706e-01, -1.93299908e-01,\n",
       "         6.26936430e-02,  6.87205956e-03],\n",
       "       [-2.73955094e-02,  3.33124481e-02, -2.11882484e-02,\n",
       "         6.87205956e-03,  7.53269397e-04]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a, a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.96341997e-01, -1.21153400e+00,  7.70591320e-01,\n",
       "        -2.49928609e-01, -2.73955094e-02],\n",
       "       [-1.21153400e+00,  1.47320361e+00, -9.37025225e-01,\n",
       "         3.03908706e-01,  3.33124481e-02],\n",
       "       [ 7.70591320e-01, -9.37025225e-01,  5.95991120e-01,\n",
       "        -1.93299908e-01, -2.11882484e-02],\n",
       "       [-2.49928609e-01,  3.03908706e-01, -1.93299908e-01,\n",
       "         6.26936430e-02,  6.87205956e-03],\n",
       "       [-2.73955094e-02,  3.33124481e-02, -2.11882484e-02,\n",
       "         6.87205956e-03,  7.53269397e-04]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a, a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclusion\n",
    "    - For neural networks, do not use rank 1 vectors.\n",
    "    - Instead, always use row or column vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Logistic Regression Cost Function\n",
    "- For logistic regression, we defined that $\\hat y = P(y=1|X) = \\sigma (w^{T} X + b)$.\n",
    "- In other words, we have:\n",
    "    - If $y = 1$: $p(y|X) = \\hat y$\n",
    "    - If $y = 0$: $p(y|X) = 1 - \\hat y$\n",
    "- With this, we can summarize it as:\n",
    "    - $p(y|X) = \\hat y^{y} (1 - \\hat y)^{1 - y}$\n",
    "- For a training set of *m* examples (assuming i.i.d), the Conditional Log-Likelihood function is:\n",
    "    - $LCL((w, b); y|X) = log(\\prod_{i=0}^{m} p_{(w, b)}(y^{(i)}|X^{(i)})) = \\sum_{i=0}^{m} log(p_{(w, b)}(y^{(i)}|X^{(i)})$\n",
    "- Note that $log(p_{(w, b)}(y^{(i)}|X^{(i)}) = y^{(i)}log(\\hat y^{(i)}) + (1 - y^{(i)})log(1 - \\hat y^{(i)})$, we then have:\n",
    "    - $LCL((w, b); y|X) = - \\sum_{i=0}^{m} L_{(w, b)(\\hat y^{(i)}, y^{(i)})}$\n",
    "- Thus, to find the MLE for $LCL((w, b); y|X)$, we are equivalently looking for the minimizer of $\\sum_{i=0}^{m} L_{(w, b)(\\hat y^{(i)}, y^{(i)})}$.\n",
    "- Thus, addressing the scaling issue using $\\frac{1}{m}$, we get the cost function to minimize:\n",
    "    - $J_{(w, b)}(\\hat y, y) = \\frac{1}{m} \\sum_{i=0}^{m} L_{(w, b)(\\hat y^{(i)}, y^{(i)})}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
